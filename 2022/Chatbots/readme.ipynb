{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kolomaznik/vui2/blob/main/2022/Chatbots/readme.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ChatBots "
      ],
      "metadata": {
        "id": "QMxEZLyy14HZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preƒço chatboti**\n",
        "\n",
        "Chatboty umo≈æ≈àuj√∫ podnikom **spoji≈• sa so z√°kazn√≠kmi osobnej≈°√≠m sp√¥sobom** bez n√°kladov na ƒæudsk√Ωch z√°stupcov.\n",
        "\n",
        "Odovzd√°va **probl√©my z√°kazn√≠ka ≈æivej osobe**, ak sa probl√©m stane pr√≠li≈° zlo≈æit√Ωm na to, aby ho chatbot vyrie≈°il.\n",
        "≈†etr√≠ ƒçasu a pe≈àaz√≠\n",
        "\n",
        "Chatboty dok√°≈æu **automatizova≈• √∫lohy**, ktor√© s√∫ vykon√°van√© ƒçasto a v konkr√©tnych ƒçasoch. To d√°va zamestnancom ƒças s√∫stredi≈• sa na d√¥le≈æitej≈°ie √∫lohy a z√°kazn√≠k nemus√≠ ƒçaka≈• na odpoveƒè.\n",
        "\n",
        "V minulosti sa organiz√°cie spoliehali na pas√≠vnu interakciu so z√°kazn√≠kmi a ƒçakali, k√Ωm ich z√°kazn√≠ci oslovia ako prv√≠. \n",
        "S chatbotmi m√¥≈æu organiz√°cie proakt√≠vne interagova≈•, preto≈æe roboty m√¥≈æu iniciova≈• konverz√°cie a **monitorova≈•**, ako z√°kazn√≠ci pou≈æ√≠vaj√∫ webov√© str√°nky. Organiz√°cie potom m√¥≈æu **pou≈æi≈• inform√°cie** zhroma≈æden√© z monitorovania, aby pon√∫kli konkr√©tne stimuly pre kupuj√∫cich, pomohli pou≈æ√≠vateƒæom pri navig√°cii na str√°nke a odpovedali na bud√∫ce ot√°zky.\n",
        "\n",
        "Roboty m√¥≈æu tie≈æ zaznamen√°va≈• pou≈æ√≠vateƒæsk√© √∫daje na **sledovanie spr√°vania** a n√°kupn√Ωch vzorcov, ako lep≈°ie pred√°va≈• svoje produkty a slu≈æby,\n",
        "\n",
        "Chatboty dok√°≈æu vyrie≈°i≈• obavy a ot√°zky z√°kazn√≠kov vo viacer√Ωch jazykoch. \n",
        "\n",
        "Ich **24/7** pr√≠stup umo≈æ≈àuje z√°kazn√≠kom ich pou≈æ√≠va≈• bez ohƒæadu na ƒças alebo ƒçasov√© p√°smo.\n",
        "\n",
        "**V√Ωvoj**\n",
        "\n",
        "*ELIZA*\n",
        "* 1964, MIT\n",
        "* SLIP\n",
        "* Pokus o Turingov test\n",
        "* Vyu≈æit√Ω pattern matching a substituƒçn√° metodol√≥gia\n",
        "* Il√∫zia porozumenia zo strany programu\n",
        "* Ch√Ωba kontext rozhovoru\n",
        "\n",
        "Pomocou ‚Äûporovn√°vania vzorov‚Äú a substituƒçnej metodol√≥gie poskytuje program vopred pripraven√© odpovede, vƒèaka ktor√Ωm maj√∫ pou≈æ√≠vatelia pocit, ≈æe sa rozpr√°vaj√∫ s niek√Ωm, kto pochopil ich vstup.\n",
        "Program bol obmedzen√Ω skriptami, ktor√© boli ulo≈æen√© v programe\n",
        "\n",
        "\n",
        "*PARRY*\n",
        "* 1972\n",
        "* Zn√°my rozhovor medzi terapeutom a paranoidn√≠m schizofrenikom (PARRY)\n",
        "* Pre≈°iel Turingov√Ωm testom\n",
        "\n",
        "V roku 1971 Kenneth Colby, psychiater Stanfordsk√©ho laborat√≥ria umelej inteligencie, uva≈æoval, ƒçi by poƒç√≠taƒçe mohli prispie≈• k pochopeniu funkcie mozgu. \n",
        "Veril, ≈æe poƒç√≠taƒç m√¥≈æe pom√¥c≈• pri lieƒçbe pacientov s du≈°evn√Ωmi chorobami. \n",
        "Tieto my≈°lienky viedli Colbyho k vyvinutiu Parryho, poƒç√≠taƒçov√©ho programu, ktor√Ω simuloval ƒçloveka so schizofr√©niou. \n",
        "Colby veril, ≈æe Parry m√¥≈æe pom√¥c≈• vzdel√°va≈• ≈°tudentov medic√≠ny sk√¥r, ako zaƒçn√∫ lieƒçi≈• pacientov. \n",
        "Parry bol pova≈æovan√Ω za prv√©ho chatovacieho robota, ktor√Ω pre≈°iel Turingov√Ωm testom. \n",
        "U≈æ vtedy jej vznik rozp√∫tal v√°≈ænu debatu o mo≈ænostiach umelej inteligencie.\n",
        "\n",
        "*ƒéal≈°√≠ chatboti**\n",
        "\n",
        "* KL-ONE (1974) ‚Äì reprezent√°cia znalost√≠ v s√©mantick√Ωch sie≈•ach a r√°mcoch\n",
        "* Jabberwacky (1982) ‚Äì Simul√°cia ƒæudskej konverz√°cie z√°bavnou formou\n",
        "  contextual pattern matching\n",
        "* Dr. Sbaitso (1992) ‚Äì Rozhovor so psychol√≥gom. ‚ÄûWhy do you feel that way?‚Äú\n",
        "* A.L.I.C.E. (1995) ‚Äì Online konverz√°cia ako s re√°lnym ƒçlovekom\n",
        "  * heuristic pattern matching¬†\n",
        "* SmarterChild (2001) ‚Äì MSN Messenger, predchodca Siri\n",
        "* Siri (2010)\n",
        "* Google Now (2012)\n",
        "* Cortana (2014)\n",
        "* Alexa (2014)\n",
        "* ‚Ä¶\n",
        "\n",
        "**Turing test**\n",
        "\n",
        "V roku 1950 Alan Turing, poƒç√≠taƒçov√Ω priekopn√≠k, nap√≠sal vedeck√∫ pr√°cu s n√°zvom ‚ÄûV√Ωpoƒçtov√© stroje a inteligencia‚Äú. V dokumente vedec naznaƒçil, ≈æe poƒç√≠taƒçov√Ω program m√¥≈æe myslie≈• a hovori≈• ako ƒçlovek. Turing navrhol experiment s n√°zvom Imitation Game, ktor√Ω je zn√°my ako Turingov test, aby to dok√°zal. V Turingovom experimente osoba oznaƒçen√° ako sudca ƒçetovala cez poƒç√≠taƒç s ƒçlovekom a strojom, ktor√Ωch nebolo vidie≈•, m√° za √∫lohu rozozna≈• ƒçi si p√≠≈°e so strojom alebo ƒçlovekom\n",
        "\n",
        "\n",
        "**Typy chatbotov**\n",
        "\n",
        "* rule-based\n",
        "\n",
        "  * Daj√∫ sa hravo prirovna≈• k filmov√Ωm hercom, preto≈æe sa rovnako ako oni v≈ædy dr≈æia scen√°ra. \n",
        "  * Roboty zalo≈æen√© na pravidl√°ch poskytuj√∫ odpovede na z√°klade s√∫boru pravidiel if/then, ktor√© sa m√¥≈æu l√≠≈°i≈• v zlo≈æitosti. Tieto pravidl√° definuje a implementuje n√°vrh√°r chatbotov.\n",
        "  * V tomto bode je vhodn√© doda≈•, ≈æe chatboty zalo≈æen√© na pravidl√°ch nerozumej√∫ kontextu konverz√°cie. Poskytuj√∫ zodpovedaj√∫ce odpovede len vtedy, keƒè pou≈æ√≠vatelia pou≈æij√∫ kƒæ√∫ƒçov√© slovo alebo pr√≠kaz, na ktor√Ω boli naprogramovan√≠.\n",
        "\n",
        "  * Keƒè sa robotovi zalo≈æen√©mu na pravidl√°ch polo≈æ√≠ ot√°zka ako: ‚ÄûAko m√¥≈æem obnovi≈• svoje heslo? najprv hƒæad√° zn√°me kƒæ√∫ƒçov√© slov√° vo vete. V tomto pr√≠klade s√∫ kƒæ√∫ƒçov√© slov√° ‚Äûresetova≈•‚Äú a ‚Äûheslo‚Äú. Potom tieto kƒæ√∫ƒçov√© slov√° porovn√° s odpoveƒèami dostupn√Ωmi v datab√°ze, aby poskytla odpoveƒè.\n",
        "\n",
        "  * Stoj√≠ za to zd√¥razni≈•, ≈æe konverzaƒçn√© rozhrania zalo≈æen√© na pravidl√°ch sa nedok√°≈æu pouƒçi≈• z minul√Ωch sk√∫senost√≠\n",
        "\n",
        "  * Roboty zalo≈æen√© na pravidl√°ch s√∫ najlacnej≈°ie na zostavenie a najjednoduch≈°ie sa tr√©nuj√∫.\n",
        "\n",
        "  * Vysoko ≈°pecifick√Ω a ≈°trukturovan√Ω\n",
        "  * Probl√©mom m√¥≈æe by≈• zaciklen√° konverz√°cia\n",
        "* Pattern matching alg.\n",
        "  * Hrub√° sila ‚Äì v√Ωvoj√°r mus√≠ definova≈• ka≈æd√Ω vzor a odpoveda≈• na≈à\n",
        "  * Preddefinovan√Ω slovn√≠k ‚Äì hƒæad√° kƒæ√∫ƒçov√© slov√°\n",
        "  * ELIZA\n",
        "  * Najƒçastej≈°ie pou≈æ√≠van√° met√≥da\n",
        "  * Pr√≠klad:\n",
        "    * Input: ‚ÄúI‚Äôm very happy‚Äú\n",
        "    * Rule: ‚ÄúI‚Äôm‚Äù -> ‚ÄúYou are‚Äù\n",
        "    * Response generation algorithm: ‚ÄúI am delighted to hear‚Äù\n",
        "    * Output: ‚ÄúI am delighted to hear you are happy‚Äù\n",
        "\n",
        "* AIML\n",
        "  * AIML je skratka pre Artificial Intelligence Markup Language. AIML sa pou≈æ√≠va na vytvorenie alebo prisp√¥sobenie Alicebota, ƒço je aplik√°cia chat-bot zalo≈æen√° na A.L.I.C.E. (Artificial Linguistic Internet Computer Entity) slobodn√Ω softv√©r.\n",
        "\n",
        "  * AIML obsahuje kolekciu pravidiel, ktor√© definuj√∫ konverzaƒçn√© schopnosti chatbota.\n",
        "  * ƒå√≠m viac pravidiel prid√°me do AIML ‚Äì t√Ωm inteligentnej≈°√≠ je chatbot.\n",
        "  * Jedna aplik√°cia chatbota m√¥≈æe ma≈• viacero s√°d AIML a m√¥≈æe sa spr√°va≈• odli≈°ne.\n",
        "  * Ako obmedzenie chatbota zalo≈æen√©ho na AIML, ak nie je splnen√Ω ≈æiadny vstupn√Ω vzor, ‚Äã‚Äãbot jednoducho odpovie ≈°tandardn√Ωm vyhl√°sen√≠m ‚Äûnerozumel fr√°ze‚Äú.\n",
        "\n",
        "  * \\<aiml> - r√°mcuje dokument\n",
        "  * \\<category> - oznaƒçuje kateg√≥riu \n",
        "  * \\<pattern>  - vstup\n",
        "  * \\<template> - v√Ωstup\n",
        "  * \\<random> - n√°hodn√Ω v√Ωstup\n",
        "  * \\<set> - ulo≈æenie do premennej \n",
        "  * \\<get> - zavolanie premennej \n",
        "  * \\<star> - wildcard\n",
        "\n",
        "* AI chatbots\n",
        "\n",
        "  * AI chatbot je softv√©r, ktor√Ω m√¥≈æe voƒæne komunikova≈• s pou≈æ√≠vateƒæmi. Komunikaƒçn√© aplik√°cie AI s√∫ oveƒæa lep≈°√≠mi hovorcami ako ich n√°protivky zalo≈æen√© na pravidl√°ch, preto≈æe vyu≈æ√≠vaj√∫ strojov√© uƒçenie, spracovanie prirodzen√©ho jazyka (NLP) a anal√Ωzu sentimentu.\n",
        "\n",
        "  * Strojov√© uƒçenie (ML) umo≈æ≈àuje robotom identifikova≈• vzory v pou≈æ√≠vateƒæsk√Ωch vstupoch, robi≈• rozhodnutia a uƒçi≈• sa z minul√Ωch konverz√°ci√≠\n",
        "\n",
        "  * Spracovanie prirodzen√©ho jazyka (NLP) pom√°ha robotom pochopi≈•, ako ƒæudia komunikuj√∫, a umo≈æ≈àuje im toto spr√°vanie replikova≈•. NLP im umo≈æ≈àuje pochopi≈• kontext konverz√°cie, aj keƒè niekto urob√≠ pravopisn√∫ chybu alebo pou≈æije ≈æarg√≥n.\n",
        "    * NLU alebo Natural Language Understanding je vetva NLP. Ide o strojov√© ƒç√≠tanie s porozumen√≠m a uistenie sa, ≈æe stroj rozumie skutoƒçn√©mu v√Ωznamu textu. Vedeckej≈°ie povedan√©, NLU sa odohr√°va, keƒè stroj konvertuje zadan√© √∫daje pou≈æ√≠vateƒæa (to, ƒço hovor√≠) do logickej formy, ktorej algoritmy poƒç√≠taƒça rozumej√∫. A ƒç√≠m presnej≈°√≠ a spoƒæahlivej≈°√≠ je n√°stroj AI pri identifik√°cii z√°meru pou≈æ√≠vateƒæa, t√Ωm v√Ωkonnej≈°ie bude rie≈°enie, ktor√© poh√°≈àa.\n",
        "\n",
        "    * NLG, alebo Natural Language Generation, je ƒèal≈°ou podmno≈æinou NLP, ktor√° je v podstate NLU naopak: stroj generuje logick√∫ odpoveƒè, ktor√∫ potom prev√°dza na odpoveƒè prirodzen√©ho jazyka, ktorej ƒæudsk√Ω ƒçitateƒæ ƒæahko porozumie.\n",
        "\n",
        "  * Anal√Ωza sentimentu pom√°ha chatbotom pochopi≈• em√≥cie pou≈æ√≠vateƒæov.\n",
        "\n",
        " * Komunikaƒçn√© roboty AI musia by≈• dobre vycviƒçen√© a vybaven√© preddefinovan√Ωmi odpoveƒèami, aby mohli zaƒça≈•. Ako sa v≈°ak uƒçia z minul√Ωch rozhovorov, nie je potrebn√© ich nesk√¥r manu√°lne aktualizova≈•.\n",
        "\n",
        "  * Pr√≠klad \n",
        "    * Roboty AI s√∫ s ka≈ædou konverz√°ciou inteligentnej≈°ie, ƒço znamen√°, ≈æe jednoducho odzrkadƒæuj√∫ spr√°vanie pou≈æ√≠vateƒæov.\n",
        "experimente spoloƒçnosti Microsoft s n√°zvom ‚ÄûConversational Understanding‚Äú.\n",
        "Experiment zah≈ï≈àal spustenie Tay, robota AI, na Twitteri. Tay mala chatova≈• s mileni√°lmi a dok√°za≈•, ≈æe poƒç√≠taƒçov√Ω program m√¥≈æe by≈• m√∫drej≈°√≠ pomocou ‚Äûneform√°lnych a hrav√Ωch rozhovorov‚Äú.\n",
        "Experiment uk√°zal, ≈æe predpoklady Microsoftu boli spr√°vne; v√Ωsledky experimentu v≈°ak neboli ani zƒèaleka oƒçak√°van√©. Po p√°r hodin√°ch chatovania s pou≈æ√≠vateƒæmi Twitteru Tay zaƒçala posiela≈• rasistick√© a ur√°≈æliv√© tweety vr√°tane spr√°v ako ‚ÄûHitler mal pravdu‚Äú alebo ‚Äû11. september bol internou √∫lohou‚Äú.\n",
        "\n",
        "**Kroky**\n",
        "\n",
        "1. Tokeniz√°cia: Chatbot zaƒç√≠na rozsekan√≠m textu na k√∫sky (naz√Ωvan√© aj ‚Äûtokeny‚Äú) a odstr√°nen√≠m interpunkcie\n",
        "\n",
        "2. Normaliz√°cia: Robot n√°sledne odstr√°ni detaily, ktor√© nie s√∫ relevantn√©, a prevedie slov√° na ich ‚Äûnorm√°lnu‚Äú verziu, napr√≠klad tak, ≈æe v≈°etko zmen√≠ na mal√© p√≠smen√°\n",
        "\n",
        "3. Rozozn√°vanie ent√≠t: Teraz, keƒè s√∫ v≈°etky slov√° normalizovan√©, chatbot sa sna≈æ√≠ identifikova≈•, na ktor√Ω typ veci sa odkazuje. Napr√≠klad by to identifikovalo Severn√∫ Ameriku ako miesto, 67 % ako percento a Google ako organiz√°ciu.\n",
        "\n",
        "4. Anal√Ωza z√°vislosti: V ƒèal≈°om kroku robot identifikuje √∫lohu, ktor√∫ ka≈æd√© slovo hr√° vo vete, ako je podstatn√© meno, sloveso, pr√≠davn√© meno alebo objekt.\n",
        "\n",
        "5. Generovanie: Nakoniec chatbot vygeneruje mno≈æstvo odpoved√≠ pomocou inform√°ci√≠ urƒçen√Ωch vo v≈°etk√Ωch ostatn√Ωch krokoch a vyberie najvhodnej≈°iu odpoveƒè, ktor√∫ po≈°le pou≈æ√≠vateƒæovi\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Y7O-M-W6LLOt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ChatterBot library combines language corpora, text processing, machine learning algorithms, and data storage and retrieval to allow you to build flexible chatbots.\n",
        "\n",
        "You can build an industry-specific chatbot by training it with relevant data. Additionally, the chatbot will remember user responses and continue building its internal graph structure to improve the responses that it can give."
      ],
      "metadata": {
        "id": "EBWwPtdpAhef"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ChatterBot"
      ],
      "metadata": {
        "id": "gxW4Jx5FvLUD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installs ChatterBot and its dependencies "
      ],
      "metadata": {
        "id": "9WJr9AcBByKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  pip install chatterbot==1.0.4 pytz"
      ],
      "metadata": {
        "id": "GMU9V6lHBsmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try the first conversation"
      ],
      "metadata": {
        "id": "CTBOPsNLB7v5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO name your chatbot\n",
        "chatbot_name = 'andrej'\n",
        "\n",
        "# TODO name your chatbot\n",
        "exit_conditions = (\":q\", \"quit\", \"exit\")\n"
      ],
      "metadata": {
        "id": "sicHz7QRkAFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WQ_paXGAAl_",
        "outputId": "a4965283-73be-4e73-9983-34973288db75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> :q\n"
          ]
        }
      ],
      "source": [
        "from chatterbot import ChatBot\n",
        "\n",
        "chatbot = ChatBot(chatbot_name)\n",
        "\n",
        "while True:\n",
        "    query = input(\"> \")\n",
        "    if query in exit_conditions:\n",
        "        break\n",
        "    else:\n",
        "        print(f\"ü™¥ {chatbot.get_response(query)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even if our chat-pot doesn‚Äôt have much to say yet, it‚Äôs already learning and growing.\n",
        "\n",
        "During the first run, ChatterBot created a SQLite database file where it stored all our inputs and connected them with possible responses. There should be three new files that have popped up in your working directory:\n",
        "\n",
        "\n",
        "```\n",
        "‚îú‚îÄ‚îÄ db.sqlite3\n",
        "‚îú‚îÄ‚îÄ db.sqlite3-shm\n",
        "‚îî‚îÄ‚îÄ db.sqlite3-wal\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "You can see the SQLite database on:\n",
        "https://inloop.github.io/sqlite-viewer/\n",
        "\n",
        "It is also possible to use another DB, for example MongoDB\n",
        "\n"
      ],
      "metadata": {
        "id": "rbKNSoe4C5yv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make it little smarter\n",
        "\n",
        "https://chatterbot.readthedocs.io/en/stable/training.html#training-via-list-data"
      ],
      "metadata": {
        "id": "E1gQKFl5GN-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from chatterbot import ChatBot\n",
        "from chatterbot.trainers import ListTrainer"
      ],
      "metadata": {
        "id": "KRp8R9CIDc40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot = ChatBot(chatbot_name)\n",
        "\n",
        "trainer = ListTrainer(chatbot)\n",
        "\n",
        "trainer.train([\n",
        "    \"Hi\",\n",
        "    \"Welcome, friend ü§ó\",\n",
        "])\n",
        "\n",
        "# TODO Add another phrases\n",
        "trainer.train([\n",
        "    \"\",\n",
        "    \"\",\n",
        "])\n",
        "\n",
        "while True:\n",
        "    query = input(\"> \")\n",
        "    if query in exit_conditions:\n",
        "        break\n",
        "    else:\n",
        "        print(f\"ü™¥ {chatbot.get_response(query)}\")"
      ],
      "metadata": {
        "id": "tW6BbBOXGjKV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c4d14b7-2e18-4eaa-a3ca-9d03e029a1b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rList Trainer: [##########          ] 50%"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "List Trainer: [####################] 100%\n",
            "List Trainer: [####################] 100%\n",
            "> :q\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using .train() injects entries into your database to build upon the graph structure that ChatterBot uses to choose possible replies.\n",
        "\n",
        "![Chatbot training](https://akela.mendelu.cz/~xgono/AI/chatbot_training.svg)\n",
        "\n",
        "If you pass an iterable with exactly two items to ListTrainer.train(), then ChatterBot considers the first item a statement and the second item an acceptable response.\n",
        "\n"
      ],
      "metadata": {
        "id": "4XC41TE8JrgS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessors\n",
        "\n",
        "ChatterBot‚Äôs preprocessors are simple functions that modify the input statement that a chat bot receives before the statement gets processed by the logic adaper.\n",
        "\n",
        "ChatterBot comes with several built-in preprocessors.\n",
        "\n",
        "```\n",
        "chatbot = ChatBot(\n",
        "    'Bob the Bot',\n",
        "    preprocessors=[\n",
        "        'chatterbot.preprocessors.clean_whitespace'\n",
        "    ]\n",
        ")\n",
        "```\n",
        "\n",
        "### Logic Adapters\n",
        "\n",
        "Logic adapters determine the logic for how ChatterBot selects a response to a given input statement.\n",
        "\n",
        "The logic adapter that your bot uses can be specified by setting the logic_adapters parameter to the import path of the logic adapter you want to use.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "chatbot = ChatBot(\n",
        "    \"My ChatterBot\",\n",
        "    logic_adapters=[\n",
        "        \"chatterbot.logic.BestMatch\"\n",
        "    ]\n",
        ")\n",
        "```\n",
        "\n",
        "![Chatbot training](https://akela.mendelu.cz/~xgono/AI/logic_adapters.svg)\n",
        "\n",
        "\n",
        "#### Best Match Adapter\n",
        "\n",
        "The best match adapter uses a function to compare the input statement to known statements. Once it finds the closest match to the input statement, it uses another function to select one of the known responses to that statement.\n",
        "\n",
        "\n",
        "#### Time Logic Adapter\n",
        "\n",
        "The TimeLogicAdapter identifies statements in which a question about the current time is asked. If a matching question is detected, then a response containing the current time is returned.\n",
        "\n",
        "```\n",
        "User: What time is it?\n",
        "Bot: The current time is 4:45PM.\n",
        "```\n",
        "\n",
        "#### Mathematical Evaluation Adapter\n",
        "\n",
        "The MathematicalEvaluation logic adapter checks a given statement to see if it contains a mathematical expression that can be evaluated. If one exists, then it returns a response containing the result. This adapter is able to handle any combination of word and numeric operators.\n",
        "\n",
        "```\n",
        "User: What is four plus four?\n",
        "Bot: (4 + 4) = 8\n",
        "```\n",
        "\n",
        "#### Specific Response Adapter\n",
        "\n",
        "If the input that the chat bot receives, matches the input text specified for this adapter, the specified response will be returned.\n",
        "\n",
        "```\n",
        "trainer.train([\n",
        "    'How can I help you?',\n",
        "    'I want to create a chat bot',\n",
        "    'Have you read the documentation?',\n",
        "    'No, I have not',\n",
        "    'This should help get you started: http://chatterbot.rtfd.org/en/latest/quickstart.html'\n",
        "])\n",
        "```\n",
        "\n",
        "```\n",
        "bot = ChatBot(\n",
        "    'Example Bot',\n",
        "    storage_adapter='chatterbot.storage.SQLStorageAdapter',\n",
        "    logic_adapters=[\n",
        "        {\n",
        "            'import_path': 'chatterbot.logic.BestMatch',\n",
        "            'default_response': 'I am sorry, but I do not understand.',\n",
        "            'maximum_similarity_threshold': 0.90\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "```"
      ],
      "metadata": {
        "id": "9OQot0OIzWJn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step involves searching the database for a known statement that matches or closely matches the input statement. Once a match is selected, the second step involves selecting a known response to the selected match. Frequently, there will be a number of existing statements that are responses to the known match."
      ],
      "metadata": {
        "id": "QDBMhUHP43qn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can write your own logic adapters by creating a new class that inherits from LogicAdapter and overrides the necessary methods established in the LogicAdapter base class."
      ],
      "metadata": {
        "id": "wG7qElzH7IhN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Storage Adapters and text search\n",
        "\n",
        "Storage adapters provide an interface that allows ChatterBot to connect to different storage technologies.\n",
        "\n",
        "ChatterBot‚Äôs storage adapters support text search functionality. For example Bigram Text Index\n",
        "\n",
        "#### Bigram Text Index\n",
        "\n",
        "In addition, the generation of the pairs ensures that there is a smaller number of possible matches based on the probability of finding two neighboring words in an existing string that match the search parameter.\n",
        "\n",
        "For searches in larger data sets, the bigrams also reduce the number of OR comparisons that need to occur on a database level. This will always be a reduction of n - 1 where n is the number of search words.\n",
        "\n",
        "![Chatbot training](https://akela.mendelu.cz/~xgono/AI/bigrams.svg)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yKPrFMpk7Wce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export a WhatsApp Chat"
      ],
      "metadata": {
        "id": "NeivV423LVJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "chat_file = \"https://akela.mendelu.cz/~xgono/AI/chat.txt\"\n",
        "response = requests.get(chat_file)\n",
        "open(\"chat.txt\", \"wb\").write(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcCASQGTnD1j",
        "outputId": "264ee3ec-6029-472e-ae49-b20ef48174d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9270"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can download a TXT file that contains the chat history of a WhatsApp conversation.\n",
        "\n",
        "---\n",
        "\n",
        "Example of exported WhatsApp chat\n",
        "\n",
        "```\n",
        "9/15/22, 14:50 - Messages and calls are end-to-end encrypted. No one outside of this chat, not even WhatsApp, can read or listen to them. Tap to learn more.\n",
        "9/15/22, 14:49 - Andrej: Hi Martin, Andrej here! Are you ready?\n",
        "9/15/22, 14:50 - Martin: Hello! Yes I'm ready.\n",
        "9/16/22, 06:34 - Martin: <Media omitted>\n",
        "\n",
        "```\n",
        "\n",
        "The format isn‚Äôt ideal to use for training.\n",
        "\n",
        "\n",
        "ChatterBot uses complete lines as messages when a chatbot replies to a user message. In the case of this chat export, it would therefore include all the message metadata. That means your chatbot would be studying the dates, times, and usernames! Not exactly great conversation fertilizer.\n",
        "\n"
      ],
      "metadata": {
        "id": "YnsJ_ChMLgv_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clean our chat export"
      ],
      "metadata": {
        "id": "cZwOlalCmB7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, you may notice that the first line of the provided chat export isn‚Äôt part of the conversation. Also, each actual message starts with metadata that includes a date, a time, and the username of the message sender.\n",
        "\n",
        "If you scroll further down the conversation file, you‚Äôll find lines that aren‚Äôt real messages. Because you didn‚Äôt include media files in the chat export, WhatsApp replaced these files with the text <Media omitted>.\n",
        "\n"
      ],
      "metadata": {
        "id": "UHotwh4Y9ujb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def remove_chat_metadata(chat_export_file):\n",
        "    date_time = r\"(\\d+\\/\\d+\\/\\d+,\\s\\d+:\\d+)\"  # e.g. \"9/16/22, 06:34\"\n",
        "    dash_whitespace = r\"\\s-\\s\"  # \" - \"\n",
        "    username = r\"([\\w\\s]+)\"  # e.g. \"Martin\"\n",
        "    metadata_end = r\":\\s\"  # \": \"\n",
        "    pattern = date_time + dash_whitespace + username + metadata_end\n",
        "\n",
        "    with open(chat_export_file, \"r\") as corpus_file:\n",
        "        content = corpus_file.read()\n",
        "    cleaned_corpus = re.sub(pattern, \"\", content)\n",
        "    return tuple(cleaned_corpus.split(\"\\n\"))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(remove_chat_metadata(\"chat.txt\"))"
      ],
      "metadata": {
        "id": "yx9LSFyBMePj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d48bd16-4318-41f3-ad04-49090d24aee9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('9/15/22, 14:50 - Messages and calls are end-to-end encrypted. No one outside of this chat, not even WhatsApp, can read or listen to them. Tap to learn more.', 'Hi Martin, Philipp here!', 'I‚Äôm ready to talk about plants!', \"Oh that's great!\", \"I've been waiting for a good convo about plants for a long time\", 'We all have.', 'Did you know they need water to grow?', 'I always thought that love and music was more than enough', 'But water makes sense', 'Do you talk to your plants?', 'I do!', 'Some of them even have names', 'What do they like to hear?', 'Motivational speeches', 'Or stories about plants that made it and are living outside now', 'Oohhh \\U0001f972', 'Are you training them for independence?', 'Yeah! I want them to be strong and take care of themselves at some point', \"That's heroic!\", \"Sounds like you're a great plant parent\", 'Do you have any plant care pro tips?', 'Ahh, idk i just leave them be...', 'They are doing ok but not great', 'So they are independent already!', \"I guess one tip would be to get plants that don't need much :P\", 'Like plastic plants, for example?', 'Haha, yes! Get independent plants!', 'Plastic plants are the cream of the crop', \"I haven't graduated to plastic plants yet\", 'Cream of the crop?', \"I don't know what this means\", 'Crop is a cultivated plant that is grown on a large scale commercially, especially a cereal, fruit, or vegetable', 'The cream must be the best of the best', 'Or maybe just all of it blended together? ;p', \"I don't grow any crop at home\", 'And no cream, in case you wondered', 'Ah, gotcha!', 'Let me show you something!', 'I let you', '<Media omitted>', \"It's a monsters!\", 'Monstera* (auto correct...)', 'I‚Äôm currently running an experiment of keeping my Monstera on the balcony', 'A monstera and a fluffy little monster', 'Haha, yeah, hard to tell which is which', 'How has it been going with the monstera on the balcony?', \"I've tried that too over the summer\", 'Pretty bad', 'Oh really?', 'What happened?', 'Yeah', 'I mean ‚Ä¶ it‚Äôs weird.', 'The leafs that she had are getting dryer and dryer. But she‚Äôs also growing plenty of new ones', 'It‚Äòs like she‚Äôs changing her summer jacket to a winter jacket', 'Yeah something similar happened to ours', 'It had thrips over the winter, so we needed to get rid of them', \"Ah, thrips are those tiny little beasts that eat your plants, aren't they?\", 'Thought that the balcony time would help get rid of them', 'Yeah they are horrible, really cute and tiny and deadly', \"Most of the monstera's leaves died, but now there are new ones coming\", \"I think it's the amount of light, they need different leaves for stronger sunlight\", 'Do you have any other approaches to get rid of tiny monsters?', '(Except putting the plant on the balcony)', \"Tiny monstera's?\", 'Handling tiny monsters to grow big monsteras', 'Haha', 'Well, no. Just the balcony. This worked best', 'We tried applying soapy water', 'Which is a suggestion', \"And it keeps them a bit in check, but you can't get totally rid of them\", \"10/10 thrips don't like this simple trick üòÖ\", 'Oh, okay', \"8/10 thrips don't like this simple trick\", \"I really hope they're gone now ü§û\", 'Lol', \"It's depressing when lil monstera keeps making new leaves for them just to get infected üò¢\", 'Yeah, nature can be harsh', \"Ah yes, it's a slightly strange rendering of nature though, if it's about potted plants\", 'I feel like they could handle it better out in the wild', \"Good that you're making yours strong enough to leave eventually!\", 'For the other plants, my words help them to grow', 'Inside and outside', 'What are the magic words?', 'Wait, do you make *all* plants grow???', 'Grow hastily, grow healthily, grow heartily \\U0001fa84‚ú®üå±', 'Do you not make all your plants grow?', ':))', \"What do you consider 'your plants'?\", 'Oh, now I understand!', 'I only considered the plants that live in my apartment (or my balcony) as the plant-spell-receiving plants', 'But maybe they share the words?', 'Do raindrops touch their leaves?', \"I'm sure they chat with the other plants if they can\", 'Yeah, I heard that trees communicate in the woods with their roots', 'But thinking about that makes me feel bad that my plants are potted üò¨', 'üò¢', 'Yeah...', \"The trees use mushrooms mycelium in the ground that's hooked to their roots to chat\", 'Pretty cool üòé', 'So i also have a Pilea', 'And some basil plants', \"Do you have a photo of the Pilea? I don't know how it looks\", 'And a peace lily', 'https://en.m.wikipedia.org/wiki/Pilea_peperomioides', 'Oh, the Pilea looks cool!', 'The leaves are like small umbrellas', 'Did you actually manage to keep a supermarket basil alive or did you grow it yourself?', \"that's quite a story!\", 'my dad had a flowering basil last year and put the seeds from that one plant into seeding pots', 'so many of them came up that he had about two dining room tables full of basil plants, each in their own pots...', 'i got three of them, so they are second gen supermarket basils with lots of siblings :)', 'I always thought supermarket basil was meant to die after a few days', 'But it seems it was my subpar care', 'Congratulations to your basil dynasty!', 'Thanks!', 'Kudos go mostly to my dad', 'But you can bring them through winter', \"If there's no thrip infestation...\", '<Media omitted>', '<Media omitted>', '<Media omitted>', 'Morning view of most of my house plants', 'Fingers crossed ü§û', 'Are the ones on top avocados?', 'Yes, there are a couple of seedlings that wanted to live', 'Two more on the balcony of a similar size', \"And another seed that's one it's way ü§∑\\u200d‚ôÇÔ∏è\", 'I heard that it‚Äôs not easy to raise an Avodaco!', \"I haven't even heard of Avodacos!\", 'Wait, weren‚Äôt we talking about avocados?', 'Ah yes avocados ü•ë!', 'I was just joking, riffing based on your typo üòù', 'Ooh! I was wondering why there was a red line under that word', 'Do you think there are avotacos? ü•ëüåÆ', 'But I like that term, too', 'üòÇ', 'If this is not a common term, then I want to make it common!', 'Maybe growing an avotaco plant would be a plant symbiosis between an avocado tree and some corn plants?', 'Sounds like the perfect experiment!', '')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_non_message_text(export_text_lines):\n",
        "    messages = export_text_lines[1:-1]\n",
        "\n",
        "    filter_out_msgs = (\"<Media omitted>\",)\n",
        "    return tuple((msg for msg in messages if msg not in filter_out_msgs))\n",
        "\n",
        "def clean_corpus(chat_export_file):\n",
        "    message_corpus = remove_chat_metadata(chat_export_file)\n",
        "    cleaned_corpus = remove_non_message_text(message_corpus)\n",
        "    return cleaned_corpus"
      ],
      "metadata": {
        "id": "GYCPtpxOGmla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_file = \"chat.txt\"\n",
        "\n",
        "chatbot = ChatBot(chatbot_name)\n",
        "\n",
        "trainer = ListTrainer(chatbot)\n",
        "cleaned_corpus = clean_corpus(corpus_file)\n",
        "trainer.train(cleaned_corpus)\n",
        "\n",
        "while True:\n",
        "    query = input(\"> \")\n",
        "    if query in exit_conditions:\n",
        "        break\n",
        "    else:\n",
        "        print(f\"ü™¥ {chatbot.get_response(query)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muDQ8bamDYTL",
        "outputId": "a92e5b60-e11c-49af-867a-401820961359"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List Trainer: [#                   ] 7%"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "List Trainer: [####################] 100%\n",
            "> :q\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://chatterbot.readthedocs.io/en/stable/logic/index.html#logic-adapters\n"
      ],
      "metadata": {
        "id": "iCHAAz77E9v-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ChatBot with Deep Learning Model"
      ],
      "metadata": {
        "id": "IgzQg4bNa1eg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this purpose, we will use the nltk (Natural Language Toolkit), which contains a whole bunch of tools for cleaning up text and preparing it for deep learning algorithms.\n",
        "\n",
        "‚Ä¶And keras, which is the deep learning framework we‚Äôll be using."
      ],
      "metadata": {
        "id": "yF9bfrYabWTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.optimizers import SGD\n",
        "import random"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxA5Ra-GbQr1",
        "outputId": "14144d61-aa88-45aa-dd2f-985847d6eb1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "url = \"https://akela.mendelu.cz/~xgono/AI/intents.json\"\n",
        "\n",
        "response = requests.get(url)\n",
        "open(\"intents.json\", \"wb\").write(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8LKavyEcfTi",
        "outputId": "53bc8712-0f9e-4b4a-b3f0-e1ae089982d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12310"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words=[]\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_words = ['?', '!']\n",
        "data_file = open('intents.json').read()\n",
        "intents = json.loads(data_file)"
      ],
      "metadata": {
        "id": "1jzX3ehWbffG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```\n",
        "intents\": [\n",
        "    {\n",
        "          \"tag\": \"greeting\",\n",
        "            \"patterns\": [\n",
        "                \"Hi there\",\n",
        "                \"How are you\",\n",
        "                \"Is anyone there?\",\n",
        "                \"Hey\",\n",
        "                \"Hola\",\n",
        "                \"Hello\",\n",
        "                \"Good day\",\n",
        "                \"Namaste\",\n",
        "                \"yo\"\n",
        "            ],\n",
        "            \"responses\": [\n",
        "                \"Hello\",\n",
        "                \"Good to see you again\",\n",
        "                \"Hi there, how can I help?\"\n",
        "            ],\n",
        "            \"context\": [\n",
        "                \"\"\n",
        "            ]\n",
        "        },\n",
        "    {\n",
        "            \"tag\": \"jokes\",\n",
        "            \"patterns\": [\n",
        "                \"Tell me a joke\",\n",
        "                \"Joke\",\n",
        "                \"Make me laugh\"\n",
        "            ],\n",
        "            \"responses\": [\n",
        "                \"A perfectionist walked into a bar...apparently, the bar wasn't set high enough\",\n",
        "                \"I ate a clock yesterday, it was very time-consuming\",\n",
        "                \"Never criticize someone until you've walked a mile in their shoes. That way, when you criticize them, they won't be able to hear you from that far away. Plus, you'll have their shoes.\",\n",
        "                \"The world tongue-twister champion just got arrested. I hear they're gonna give him a really tough sentence.\",\n",
        "                \"I own the world's worst thesaurus. Not only is it awful, it's awful.\",\n",
        "                \"What did the traffic light say to the car? \\\"Don't look now, I'm changing.\\\"\",\n",
        "                \"What do you call a snowman with a suntan? A puddle.\",\n",
        "                \"How does a penguin build a house? Igloos it together\",\n",
        "                \"I went to see the doctor about my short-term memory problems ‚Äì the first thing he did was make me pay in advance\",\n",
        "                \"As I get older and I remember all the people I‚Äôve lost along the way, I think to myself, maybe a career as a tour guide wasn‚Äôt for me.\",\n",
        "                \"o what if I don't know what 'Armageddon' means? It's not the end of the world.\"\n",
        "            ],\n",
        "            \"context\": [\n",
        "                \"jokes\"\n",
        "            ]\n",
        "        },\n",
        "]\n",
        "    \n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Aj4nkkL0eP1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for intent in intents['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "\n",
        "        # take each word and tokenize it\n",
        "        w = nltk.word_tokenize(pattern)\n",
        "        words.extend(w)\n",
        "        \n",
        "        # adding documents\n",
        "        documents.append((w, intent['tag']))\n",
        "\n",
        "        # adding classes to our class list\n",
        "        if intent['tag'] not in classes:\n",
        "            classes.append(intent['tag'])\n",
        "\n",
        "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
        "words = sorted(list(set(words)))\n",
        "\n",
        "classes = sorted(list(set(classes)))\n",
        "\n",
        "print (len(documents), \"documents\", documents)\n",
        "\n",
        "print (len(classes), \"classes\", classes)\n",
        "\n",
        "print (len(words), \"unique lemmatized words\", words)\n",
        "\n",
        "\n",
        "pickle.dump(words,open('words.pkl','wb'))\n",
        "pickle.dump(classes,open('classes.pkl','wb'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5MgJ0G6d5sI",
        "outputId": "0d21990d-e974-4c6f-d01d-703539bec023"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "113 documents [(['google'], 'google'), (['search'], 'google'), (['internet'], 'google'), (['Hi', 'there'], 'greeting'), (['How', 'are', 'you'], 'greeting'), (['Is', 'anyone', 'there', '?'], 'greeting'), (['Hey'], 'greeting'), (['Hola'], 'greeting'), (['Hello'], 'greeting'), (['Good', 'day'], 'greeting'), (['Namaste'], 'greeting'), (['yo'], 'greeting'), (['Bye'], 'goodbye'), (['See', 'you', 'later'], 'goodbye'), (['Goodbye'], 'goodbye'), (['Get', 'lost'], 'goodbye'), (['Till', 'next', 'time'], 'goodbye'), (['bbye'], 'goodbye'), (['Thanks'], 'thanks'), (['Thank', 'you'], 'thanks'), (['That', \"'s\", 'helpful'], 'thanks'), (['Awesome', ',', 'thanks'], 'thanks'), (['Thanks', 'for', 'helping', 'me'], 'thanks'), (['How', 'you', 'could', 'help', 'me', '?'], 'options'), (['What', 'you', 'can', 'do', '?'], 'options'), (['What', 'help', 'you', 'provide', '?'], 'options'), (['How', 'you', 'can', 'be', 'helpful', '?'], 'options'), (['What', 'support', 'is', 'offered'], 'options'), (['Tell', 'me', 'a', 'joke'], 'jokes'), (['Joke'], 'jokes'), (['Make', 'me', 'laugh'], 'jokes'), (['Who', 'are', 'you'], 'Identity'), (['what', 'are', 'you'], 'Identity'), (['What', 'is', 'the', 'time'], 'datetime'), (['what', 'is', 'the', 'date'], 'datetime'), (['date'], 'datetime'), (['time'], 'datetime'), (['tell', 'me', 'the', 'date'], 'datetime'), (['day'], 'datetime'), (['what', 'day', 'is', 'is', 'today'], 'datetime'), (['Whats', 'up'], 'whatsup'), (['Wazzup'], 'whatsup'), (['How', 'are', 'you'], 'whatsup'), (['sup'], 'whatsup'), (['How', 'you', 'doing'], 'whatsup'), (['haha'], 'haha'), (['lol'], 'haha'), (['rofl'], 'haha'), (['lmao'], 'haha'), (['thats', 'funny'], 'haha'), (['Who', 'made', 'you'], 'programmer'), (['who', 'designed', 'you'], 'programmer'), (['who', 'programmed', 'you'], 'programmer'), (['you', 'are', 'dumb'], 'insult'), (['shut', 'up'], 'insult'), (['idiot'], 'insult'), (['what', 'are', 'you', 'doing'], 'activity'), (['what', 'are', 'you', 'upto'], 'activity'), (['Awesome'], 'exclaim'), (['Great'], 'exclaim'), (['I', 'know'], 'exclaim'), (['ok'], 'exclaim'), (['yeah'], 'exclaim'), (['temperature'], 'weather'), (['weather'], 'weather'), (['how', 'hot', 'is', 'it'], 'weather'), (['who', 'is', 'he'], 'karan'), (['who', 'is', 'that'], 'karan'), (['who', 'is', 'karan'], 'karan'), (['karan', 'malik'], 'karan'), (['contact', 'developer'], 'contact'), (['contact', 'karan'], 'contact'), (['contact', 'programmer'], 'contact'), (['contact', 'creator'], 'contact'), (['You', 'are', 'awesome'], 'appreciate'), (['you', 'are', 'the', 'best'], 'appreciate'), (['you', 'are', 'great'], 'appreciate'), (['you', 'are', 'good'], 'appreciate'), (['it', 'was', 'nice', 'talking', 'to', 'you'], 'nicetty'), (['good', 'talk'], 'nicetty'), (['no'], 'no'), (['nope'], 'no'), (['news'], 'news'), (['latest', 'news'], 'news'), (['india', 'news'], 'news'), (['who', 'inspires', 'you'], 'inspire'), (['who', 'is', 'your', 'inspiration'], 'inspire'), (['who', 'motivates', 'you'], 'inspire'), (['current', 'cricket', 'matches'], 'cricket'), (['cricket', 'score'], 'cricket'), (['top', 'songs'], 'song'), (['best', 'songs'], 'song'), (['hot', 'songs'], 'song'), (['top', '10', 'songs'], 'song'), (['top', 'ten', 'songs'], 'song'), (['i', 'am', 'good'], 'greetreply'), (['I', \"'m\", 'good'], 'greetreply'), (['i', 'am', 'fine'], 'greetreply'), (['i', \"'m\", 'fine'], 'greetreply'), (['good'], 'greetreply'), (['set', 'a', 'timer'], 'timer'), (['covid', '19'], 'covid19'), (['you', 'are', 'useless'], 'suggest'), (['useless'], 'suggest'), (['suggest'], 'suggest'), (['suggestions'], 'suggest'), (['you', 'are', 'bad'], 'suggest'), (['Ask', 'me', 'a', 'riddle'], 'riddle'), (['Ask', 'me', 'a', 'question'], 'riddle'), (['Riddle'], 'riddle'), (['how', 'old', 'are', 'you'], 'age'), (['when', 'were', 'you', 'made'], 'age'), (['what', 'is', 'your', 'age'], 'age')]\n",
            "30 classes ['Identity', 'activity', 'age', 'appreciate', 'contact', 'covid19', 'cricket', 'datetime', 'exclaim', 'goodbye', 'google', 'greeting', 'greetreply', 'haha', 'inspire', 'insult', 'jokes', 'karan', 'news', 'nicetty', 'no', 'options', 'programmer', 'riddle', 'song', 'suggest', 'thanks', 'timer', 'weather', 'whatsup']\n",
            "130 unique lemmatized words [\"'m\", \"'s\", ',', '10', '19', 'a', 'age', 'am', 'anyone', 'are', 'ask', 'awesome', 'bad', 'bbye', 'be', 'best', 'bye', 'can', 'contact', 'could', 'covid', 'creator', 'cricket', 'current', 'date', 'day', 'designed', 'developer', 'do', 'doing', 'dumb', 'fine', 'for', 'funny', 'get', 'good', 'goodbye', 'google', 'great', 'haha', 'he', 'hello', 'help', 'helpful', 'helping', 'hey', 'hi', 'hola', 'hot', 'how', 'i', 'idiot', 'india', 'inspiration', 'inspires', 'internet', 'is', 'it', 'joke', 'karan', 'know', 'later', 'latest', 'laugh', 'lmao', 'lol', 'lost', 'made', 'make', 'malik', 'match', 'me', 'motivates', 'namaste', 'news', 'next', 'nice', 'no', 'nope', 'offered', 'ok', 'old', 'programmed', 'programmer', 'provide', 'question', 'riddle', 'rofl', 'score', 'search', 'see', 'set', 'shut', 'song', 'suggest', 'suggestion', 'sup', 'support', 'talk', 'talking', 'tell', 'temperature', 'ten', 'thank', 'thanks', 'that', 'thats', 'the', 'there', 'till', 'time', 'timer', 'to', 'today', 'top', 'up', 'upto', 'useless', 'wa', 'wazzup', 'weather', 'were', 'what', 'whats', 'when', 'who', 'yeah', 'yo', 'you', 'your']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building the Deep Learning Model"
      ],
      "metadata": {
        "id": "LKHJKL88f_sG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initializing training data\n",
        "training = []\n",
        "output_empty = [0] * len(classes)\n",
        "\n",
        "for doc in documents:\n",
        "\n",
        "    # initializing bag of words\n",
        "    bag = []\n",
        "\n",
        "    # list of tokenized words for the pattern\n",
        "    pattern_words = doc[0]\n",
        "\n",
        "    # lemmatize each word - create base word, in attempt to represent related words\n",
        "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
        "\n",
        "    # create our bag of words array with 1, if word match found in current pattern\n",
        "    for w in words:\n",
        "        bag.append(1) if w in pattern_words else bag.append(0)\n",
        "\n",
        "    # output is a '0' for each tag and '1' for current tag (for each pattern)\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] = 1\n",
        "\n",
        "    training.append([bag, output_row])\n",
        "    \n",
        "# shuffle our features and turn into np.array\n",
        "random.shuffle(training)\n",
        "training = np.array(training)\n",
        "\n",
        "# create train and test lists. X - patterns, Y - intents\n",
        "train_x = list(training[:,0])\n",
        "train_y = list(training[:,1])\n",
        "print(\"Training data created\")"
      ],
      "metadata": {
        "id": "_VArSjjBgD4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **Dropout** effect is that the network becomes less sensitive to the specific weights of neurons. This, in turn, results in a network capable of better generalization and less likely to overfit the training data. Dropout is only used during the training of a model and is not used when evaluating the skill of the model. \n",
        "\n",
        "\n",
        "**Sigmoid** is used for binary classification methods where we only have 2 classes, while SoftMax applies to multiclass problems. In fact, the SoftMax function is an extension of the Sigmoid function."
      ],
      "metadata": {
        "id": "vnn6W4GPAoGD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qST2l4gpA2AZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model - 3 layers. First layer 128 neurons, second layer 64 neurons and 3rd output layer contains number of neurons\n",
        "# equal to number of intents to predict output intent with softmax\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
        "\n",
        "# Compile model. Stochastic gradient descent with Nesterov accelerated gradient gives good results for this model\n",
        "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "#fitting and saving the model\n",
        "hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)\n",
        "model.save('chatbot_model.h5', hist)\n",
        "\n",
        "print(\"model created\")"
      ],
      "metadata": {
        "id": "HZS2pU-niFG0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aea866ae-d88c-4e6f-e85b-54c0b7403a6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "23/23 [==============================] - 1s 2ms/step - loss: 3.4204 - accuracy: 0.0265 \n",
            "Epoch 2/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 3.3743 - accuracy: 0.0708\n",
            "Epoch 3/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 3.3120 - accuracy: 0.1150\n",
            "Epoch 4/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 3.2850 - accuracy: 0.0885\n",
            "Epoch 5/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 3.2266 - accuracy: 0.1239\n",
            "Epoch 6/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 3.1754 - accuracy: 0.1150\n",
            "Epoch 7/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 3.0958 - accuracy: 0.1770\n",
            "Epoch 8/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 3.0300 - accuracy: 0.1947\n",
            "Epoch 9/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 2.9965 - accuracy: 0.2212\n",
            "Epoch 10/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 2.8011 - accuracy: 0.2566\n",
            "Epoch 11/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 2.7981 - accuracy: 0.2389\n",
            "Epoch 12/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 2.6042 - accuracy: 0.2832\n",
            "Epoch 13/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 2.4826 - accuracy: 0.3628\n",
            "Epoch 14/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 2.4061 - accuracy: 0.3274\n",
            "Epoch 15/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 2.2771 - accuracy: 0.3363\n",
            "Epoch 16/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 2.1129 - accuracy: 0.3894\n",
            "Epoch 17/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 2.1121 - accuracy: 0.3628\n",
            "Epoch 18/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 2.0035 - accuracy: 0.4867\n",
            "Epoch 19/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.8135 - accuracy: 0.5044\n",
            "Epoch 20/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.8223 - accuracy: 0.5398\n",
            "Epoch 21/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.6906 - accuracy: 0.5044\n",
            "Epoch 22/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.7010 - accuracy: 0.4867\n",
            "Epoch 23/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.6516 - accuracy: 0.5221\n",
            "Epoch 24/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.4882 - accuracy: 0.5752\n",
            "Epoch 25/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.5005 - accuracy: 0.6018\n",
            "Epoch 26/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.3053 - accuracy: 0.6460\n",
            "Epoch 27/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.3088 - accuracy: 0.6549\n",
            "Epoch 28/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.3886 - accuracy: 0.5575\n",
            "Epoch 29/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.3505 - accuracy: 0.6106\n",
            "Epoch 30/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.2025 - accuracy: 0.6814\n",
            "Epoch 31/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.1931 - accuracy: 0.6991\n",
            "Epoch 32/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.0260 - accuracy: 0.6991\n",
            "Epoch 33/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.9022 - accuracy: 0.7522\n",
            "Epoch 34/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.9174 - accuracy: 0.7257\n",
            "Epoch 35/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.8631 - accuracy: 0.7611\n",
            "Epoch 36/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.9381 - accuracy: 0.7345\n",
            "Epoch 37/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.9992 - accuracy: 0.6991\n",
            "Epoch 38/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.9319 - accuracy: 0.7257\n",
            "Epoch 39/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.6040 - accuracy: 0.8496\n",
            "Epoch 40/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.8945 - accuracy: 0.7168\n",
            "Epoch 41/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.6270 - accuracy: 0.8496\n",
            "Epoch 42/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.6710 - accuracy: 0.8053\n",
            "Epoch 43/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.6871 - accuracy: 0.7965\n",
            "Epoch 44/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.6733 - accuracy: 0.7965\n",
            "Epoch 45/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.7189 - accuracy: 0.7434\n",
            "Epoch 46/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.5892 - accuracy: 0.8673\n",
            "Epoch 47/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.6215 - accuracy: 0.8319\n",
            "Epoch 48/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.6281 - accuracy: 0.8319\n",
            "Epoch 49/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.7044 - accuracy: 0.7611\n",
            "Epoch 50/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.5173 - accuracy: 0.9027\n",
            "Epoch 51/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.6226 - accuracy: 0.8230\n",
            "Epoch 52/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.4783 - accuracy: 0.8584\n",
            "Epoch 53/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.4714 - accuracy: 0.8407\n",
            "Epoch 54/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.4888 - accuracy: 0.8761\n",
            "Epoch 55/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.4837 - accuracy: 0.8584\n",
            "Epoch 56/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.5505 - accuracy: 0.8230\n",
            "Epoch 57/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.5028 - accuracy: 0.8673\n",
            "Epoch 58/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3121 - accuracy: 0.9292\n",
            "Epoch 59/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.5306 - accuracy: 0.8407\n",
            "Epoch 60/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.4041 - accuracy: 0.8850\n",
            "Epoch 61/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3421 - accuracy: 0.9204\n",
            "Epoch 62/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.4400 - accuracy: 0.8584\n",
            "Epoch 63/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.4921 - accuracy: 0.8496\n",
            "Epoch 64/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.4153 - accuracy: 0.8673\n",
            "Epoch 65/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.4708 - accuracy: 0.8673\n",
            "Epoch 66/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3711 - accuracy: 0.9115\n",
            "Epoch 67/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4309 - accuracy: 0.8850\n",
            "Epoch 68/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.4062 - accuracy: 0.8673\n",
            "Epoch 69/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3209 - accuracy: 0.8938\n",
            "Epoch 70/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2834 - accuracy: 0.9115\n",
            "Epoch 71/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2787 - accuracy: 0.9292\n",
            "Epoch 72/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.4768 - accuracy: 0.8407\n",
            "Epoch 73/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3596 - accuracy: 0.9027\n",
            "Epoch 74/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.4168 - accuracy: 0.8407\n",
            "Epoch 75/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3124 - accuracy: 0.8938\n",
            "Epoch 76/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3727 - accuracy: 0.8496\n",
            "Epoch 77/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3681 - accuracy: 0.8938\n",
            "Epoch 78/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.4184 - accuracy: 0.8673\n",
            "Epoch 79/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2916 - accuracy: 0.9292\n",
            "Epoch 80/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2922 - accuracy: 0.9115\n",
            "Epoch 81/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1819 - accuracy: 0.9646\n",
            "Epoch 82/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2646 - accuracy: 0.9204\n",
            "Epoch 83/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2765 - accuracy: 0.9381\n",
            "Epoch 84/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2714 - accuracy: 0.9204\n",
            "Epoch 85/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3946 - accuracy: 0.8938\n",
            "Epoch 86/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2827 - accuracy: 0.9204\n",
            "Epoch 87/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3060 - accuracy: 0.9204\n",
            "Epoch 88/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2090 - accuracy: 0.9292\n",
            "Epoch 89/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3084 - accuracy: 0.9204\n",
            "Epoch 90/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2512 - accuracy: 0.9381\n",
            "Epoch 91/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3162 - accuracy: 0.9204\n",
            "Epoch 92/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3534 - accuracy: 0.8938\n",
            "Epoch 93/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3500 - accuracy: 0.8850\n",
            "Epoch 94/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2492 - accuracy: 0.9204\n",
            "Epoch 95/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3001 - accuracy: 0.9292\n",
            "Epoch 96/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3547 - accuracy: 0.8673\n",
            "Epoch 97/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2322 - accuracy: 0.9027\n",
            "Epoch 98/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2196 - accuracy: 0.9292\n",
            "Epoch 99/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1611 - accuracy: 0.9646\n",
            "Epoch 100/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2349 - accuracy: 0.9292\n",
            "Epoch 101/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2842 - accuracy: 0.9204\n",
            "Epoch 102/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2027 - accuracy: 0.9204\n",
            "Epoch 103/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2298 - accuracy: 0.9292\n",
            "Epoch 104/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1724 - accuracy: 0.9646\n",
            "Epoch 105/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1730 - accuracy: 0.9558\n",
            "Epoch 106/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2482 - accuracy: 0.9381\n",
            "Epoch 107/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2253 - accuracy: 0.9381\n",
            "Epoch 108/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3334 - accuracy: 0.8938\n",
            "Epoch 109/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2383 - accuracy: 0.9292\n",
            "Epoch 110/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2936 - accuracy: 0.8761\n",
            "Epoch 111/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2410 - accuracy: 0.9469\n",
            "Epoch 112/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2326 - accuracy: 0.9027\n",
            "Epoch 113/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2382 - accuracy: 0.9469\n",
            "Epoch 114/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2698 - accuracy: 0.8850\n",
            "Epoch 115/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1720 - accuracy: 0.9646\n",
            "Epoch 116/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2284 - accuracy: 0.8938\n",
            "Epoch 117/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2406 - accuracy: 0.9292\n",
            "Epoch 118/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1518 - accuracy: 0.9558\n",
            "Epoch 119/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2282 - accuracy: 0.9381\n",
            "Epoch 120/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1901 - accuracy: 0.9381\n",
            "Epoch 121/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1927 - accuracy: 0.9381\n",
            "Epoch 122/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1641 - accuracy: 0.9292\n",
            "Epoch 123/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1707 - accuracy: 0.9558\n",
            "Epoch 124/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1442 - accuracy: 0.9646\n",
            "Epoch 125/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2611 - accuracy: 0.8850\n",
            "Epoch 126/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0950 - accuracy: 0.9823\n",
            "Epoch 127/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1440 - accuracy: 0.9735\n",
            "Epoch 128/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1827 - accuracy: 0.9646\n",
            "Epoch 129/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1699 - accuracy: 0.9735\n",
            "Epoch 130/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1939 - accuracy: 0.9204\n",
            "Epoch 131/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1049 - accuracy: 0.9735\n",
            "Epoch 132/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2365 - accuracy: 0.9292\n",
            "Epoch 133/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1644 - accuracy: 0.9558\n",
            "Epoch 134/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2132 - accuracy: 0.9292\n",
            "Epoch 135/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1988 - accuracy: 0.9469\n",
            "Epoch 136/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1966 - accuracy: 0.9558\n",
            "Epoch 137/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1791 - accuracy: 0.9469\n",
            "Epoch 138/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1811 - accuracy: 0.9292\n",
            "Epoch 139/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1144 - accuracy: 0.9823\n",
            "Epoch 140/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1250 - accuracy: 0.9823\n",
            "Epoch 141/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1542 - accuracy: 0.9558\n",
            "Epoch 142/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2489 - accuracy: 0.9381\n",
            "Epoch 143/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1980 - accuracy: 0.9292\n",
            "Epoch 144/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1418 - accuracy: 0.9558\n",
            "Epoch 145/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2110 - accuracy: 0.9292\n",
            "Epoch 146/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1851 - accuracy: 0.9469\n",
            "Epoch 147/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1503 - accuracy: 0.9469\n",
            "Epoch 148/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1182 - accuracy: 0.9646\n",
            "Epoch 149/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1505 - accuracy: 0.9558\n",
            "Epoch 150/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1600 - accuracy: 0.9381\n",
            "Epoch 151/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1750 - accuracy: 0.9204\n",
            "Epoch 152/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1609 - accuracy: 0.9381\n",
            "Epoch 153/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1557 - accuracy: 0.9558\n",
            "Epoch 154/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2084 - accuracy: 0.9292\n",
            "Epoch 155/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0947 - accuracy: 0.9823\n",
            "Epoch 156/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1852 - accuracy: 0.9292\n",
            "Epoch 157/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1603 - accuracy: 0.9735\n",
            "Epoch 158/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1883 - accuracy: 0.9292\n",
            "Epoch 159/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1221 - accuracy: 0.9558\n",
            "Epoch 160/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0766 - accuracy: 0.9735\n",
            "Epoch 161/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1084 - accuracy: 0.9646\n",
            "Epoch 162/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0694 - accuracy: 0.9735\n",
            "Epoch 163/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1791 - accuracy: 0.9469\n",
            "Epoch 164/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1382 - accuracy: 0.9646\n",
            "Epoch 165/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2159 - accuracy: 0.9204\n",
            "Epoch 166/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0962 - accuracy: 0.9646\n",
            "Epoch 167/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2203 - accuracy: 0.9204\n",
            "Epoch 168/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2207 - accuracy: 0.9381\n",
            "Epoch 169/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1196 - accuracy: 0.9735\n",
            "Epoch 170/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1823 - accuracy: 0.9292\n",
            "Epoch 171/200\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.1250 - accuracy: 0.9646\n",
            "Epoch 172/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1463 - accuracy: 0.9469\n",
            "Epoch 173/200\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.1207 - accuracy: 0.9646\n",
            "Epoch 174/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1361 - accuracy: 0.9558\n",
            "Epoch 175/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1023 - accuracy: 0.9558\n",
            "Epoch 176/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0990 - accuracy: 0.9735\n",
            "Epoch 177/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.2039 - accuracy: 0.9381\n",
            "Epoch 178/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1861 - accuracy: 0.9469\n",
            "Epoch 179/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1687 - accuracy: 0.9381\n",
            "Epoch 180/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.2757 - accuracy: 0.8850\n",
            "Epoch 181/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1524 - accuracy: 0.9381\n",
            "Epoch 182/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0881 - accuracy: 0.9823\n",
            "Epoch 183/200\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.2043 - accuracy: 0.9558\n",
            "Epoch 184/200\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.1249 - accuracy: 0.9558\n",
            "Epoch 185/200\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.1379 - accuracy: 0.9646\n",
            "Epoch 186/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1443 - accuracy: 0.9646\n",
            "Epoch 187/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1300 - accuracy: 0.9646\n",
            "Epoch 188/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1203 - accuracy: 0.9469\n",
            "Epoch 189/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0617 - accuracy: 0.9912\n",
            "Epoch 190/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1656 - accuracy: 0.9558\n",
            "Epoch 191/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1163 - accuracy: 0.9558\n",
            "Epoch 192/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0854 - accuracy: 0.9912\n",
            "Epoch 193/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1055 - accuracy: 0.9735\n",
            "Epoch 194/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1444 - accuracy: 0.9646\n",
            "Epoch 195/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1270 - accuracy: 0.9646\n",
            "Epoch 196/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1194 - accuracy: 0.9558\n",
            "Epoch 197/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1751 - accuracy: 0.9381\n",
            "Epoch 198/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1023 - accuracy: 0.9646\n",
            "Epoch 199/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1256 - accuracy: 0.9558\n",
            "Epoch 200/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1606 - accuracy: 0.9469\n",
            "model created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we have our training and test data ready and we can now use a deep learning model from keras called Sequential.\n",
        "\n",
        "The Sequential model in keras is actually one of the simplest neural networks, a multi-layer perceptron. This particular network has 3 layers, with the first one having 128 neurons, the second one having 64 neurons, and the third one having the number of intents as the number of neurons. Remember, the point of this network is to be able to predict which intent to choose given some data.\n",
        "\n",
        "The model will be trained with stochastic gradient descent. Stochastic gradient descent is more efficient than normal gradient descent.\n",
        "\n",
        "After the model is trained, the whole thing is turned into a numpy array and saved as chatbot_model.h5.\n",
        "\n",
        "We will use this model to form our chatbot interface."
      ],
      "metadata": {
        "id": "8n_kUh1IjvVT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create"
      ],
      "metadata": {
        "id": "PPRJTeuxl5Y7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('chatbot_model.h5')\n",
        "import json\n",
        "import random\n",
        "intents = json.loads(open('intents.json').read())\n",
        "words = pickle.load(open('words.pkl','rb'))\n",
        "classes = pickle.load(open('classes.pkl','rb'))"
      ],
      "metadata": {
        "id": "J2rybWSCmEwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_up_sentence(sentence):\n",
        "    sentence_words = nltk.word_tokenize(sentence)\n",
        "    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
        "    return sentence_words\n",
        "\n",
        "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
        "\n",
        "def bow(sentence, words, show_details=True):\n",
        "    # tokenize the pattern\n",
        "    sentence_words = clean_up_sentence(sentence)\n",
        "    # bag of words - matrix of N words, vocabulary matrix\n",
        "    bag = [0]*len(words)\n",
        "    for s in sentence_words:\n",
        "        for i,w in enumerate(words):\n",
        "            if w == s:\n",
        "                # assign 1 if current word is in the vocabulary position\n",
        "                bag[i] = 1\n",
        "                if show_details:\n",
        "                    print (\"found in bag: %s\" % w)\n",
        "    return(np.array(bag))\n",
        "\n",
        "def predict_class(sentence, model):\n",
        "    # filter out predictions below a threshold\n",
        "    p = bow(sentence, words,show_details=False)\n",
        "    res = model.predict(np.array([p]))[0]\n",
        "    ERROR_THRESHOLD = 0.25\n",
        "    results = [[i,r] for i,r in enumerate(res) if r>ERROR_THRESHOLD]\n",
        "    # sort by strength of probability\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return_list = []\n",
        "    for r in results:\n",
        "        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n",
        "    return return_list\n",
        "\n",
        "def getResponse(ints, intents_json):\n",
        "    tag = ints[0]['intent']\n",
        "    list_of_intents = intents_json['intents']\n",
        "    for i in list_of_intents:\n",
        "        if(i['tag']== tag):\n",
        "            result = random.choice(i['responses'])\n",
        "            break\n",
        "    return result\n",
        "\n",
        "def chatbot_response(msg):\n",
        "    ints = predict_class(msg, model)\n",
        "    res = getResponse(ints, intents)\n",
        "    return res"
      ],
      "metadata": {
        "id": "a4hO7nHWnBOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    query = input(\"> \")\n",
        "    if query in exit_conditions:\n",
        "        break\n",
        "    else:\n",
        "        res = chatbot_response(query)\n",
        "        print(res)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gh-qAdiYnGeP",
        "outputId": "412b524e-0f1f-427a-8a76-3952f8d7aa45"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> :q\n"
          ]
        }
      ]
    }
  ]
}